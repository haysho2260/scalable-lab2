{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**\n",
        "\n",
        "Features in the notebook:\n",
        "1. Uses Maxime Labonne's [FineTome 100K](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset.\n",
        "1. Convert ShareGPT to HuggingFace format via `standardize_sharegpt`\n",
        "2. Train on Completions / Assistant only via `train_on_responses_only`\n",
        "3. Unsloth now supports Torch 2.4, all TRL & Xformers versions & Python 3.12!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scA93RzdCVyT"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"huggingface-hub>=0.34.0,<1.0\" \"transformers>=4.36,<4.58\"\n",
        "\n",
        "!pip install -q unsloth wandb python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV6YjwPqD8hi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "from huggingface_hub import HfFolder, login\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    IN_COLAB = True\n",
        "    IN_KAGGLE = False\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    # Check if running on Kaggle\n",
        "    IN_KAGGLE = os.path.exists('/kaggle')\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "    from IPython.display import clear_output\n",
        "    !pip install unsloth\n",
        "    !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git\n",
        "    !pip install wandb\n",
        "    clear_output()\n",
        "    \n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    HF_TOKEN = userdata.get('hf_token')\n",
        "    WANDB_API_KEY = userdata.get('wandb_token')\n",
        "    \n",
        "elif IN_KAGGLE:\n",
        "    print(\"Running on Kaggle\")\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    secrets = UserSecretsClient()\n",
        "    \n",
        "    HF_TOKEN = secrets.get_secret('hf_token')\n",
        "    WANDB_API_KEY = secrets.get_secret('wandb_token')\n",
        "    \n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    \n",
        "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "    WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
        "\n",
        "# Login\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "    HfFolder.save_token(HF_TOKEN)\n",
        "\n",
        "if WANDB_API_KEY:\n",
        "    wandb.login(key=WANDB_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WandB Project Configuration\n",
        "import datetime\n",
        "\n",
        "# Project name for WandB tracking\n",
        "WANDB_PROJECT = \"uncategorized\"\n",
        "\n",
        "# Optional: Add timestamp to run names for easy identification\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "\n",
        "print(f\"üìä WandB Project: {WANDB_PROJECT}\")\n",
        "print(f\"üïê Timestamp: {TIMESTAMP}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzJgTEPfL2UW"
      },
      "outputs": [],
      "source": [
        "import os, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP41L4w9f28Y"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    output_dir = \"/content/drive/MyDrive/my-model-checkpoints\"\n",
        "elif IN_KAGGLE:\n",
        "    output_dir = \"/kaggle/working/my-model-checkpoints\"\n",
        "else:\n",
        "    output_dir = \"my-model-checkpoints\"\n",
        "    \n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "eb7b2b25d5114ffe9e4e01c65d027b67",
            "859345a12282417db419c1d2260bd534",
            "b391f372a4d547079d00d51a383ad370",
            "65feeb6f753647e492375e2cd855496e",
            "215fef8284e748a39829d266b0a242f6",
            "444392dd785b42778bab7966c8bf8416",
            "e395b822390f4b8c897bfa34a56a0e36",
            "861e2487a72645bd96e79729ddf58839",
            "d30ec7f1cacf47559ec299b5da611a7c",
            "178b697d1b674b37bbdb1ef796da3b0f",
            "d82a8fd44d1f4cb983b45c02073bef34"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "f0cc7fc2-a002-4d68-fe18-d045841aa76e"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 768 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        "    device_map={\"\": \"cuda:0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "2591dc4a-641c-4e05-abd6-19c960d9087c"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Llama-3.1` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Llama-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "I'm great thanks!<|eot_id|>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")\n",
        "\n",
        "# For hyperparameter sweeps on Colab T4 free tier:\n",
        "# Use 15% of data (~12k samples) to fit multiple runs in 12-hour session\n",
        "# For final training with best params, use full dataset\n",
        "USE_FULL_DATASET = False  # Set to True for final run with best hyperparameters\n",
        "\n",
        "if not USE_FULL_DATASET:\n",
        "    print(\"Using 15% of dataset for hyperparameter sweep (Colab T4 optimized)\")\n",
        "    data = data.train_test_split(test_size=0.85, seed=42)[\"train\"]\n",
        "else:\n",
        "    print(\"Using full dataset\")\n",
        "\n",
        "splits = data.train_test_split(test_size=0.1, seed=42)\n",
        "train_valid = splits[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "dataset = train_valid[\"train\"]\n",
        "valid_dataset = train_valid[\"test\"]\n",
        "test_dataset = splits[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "formatting_func_cell"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"qwen-3\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = []\n",
        "    for convo in convos:\n",
        "        txt = tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        # Truncate BEFORE tokenizing\n",
        "        ids = tokenizer(txt, truncation=True, max_length=max_seq_length)[\"input_ids\"]\n",
        "        texts.append(tokenizer.decode(ids))\n",
        "    return { \"text\": texts }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "to\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8ebe513193ad438f86fb9f1fa821a0ff",
            "38f1f9c97517438c96d3635f19bea369",
            "919368811ac242cd9b9257376e12874b",
            "7f8601ca5a8342d78ac8dbb66accb814",
            "523d2b505ef443e6b29f52453d6fd608",
            "481d140728da40108b1b44c700274261",
            "83dc1227b69b448e8d24f16e2f9dfea5",
            "e722c1d8c8f1498fa16bdf5cc65d4b61",
            "9e6dd230283642a195e288db0798f04b",
            "bb0742afecdd41b8842f039146523551",
            "1d2be03f2b2d4951bfdf870bc7d677f8"
          ]
        },
        "id": "oPXzJZzHEgXe",
        "outputId": "d50d614c-9113-4ad3-97db-21c9a8f2d15a"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "We look at how the conversations are structured for item 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFzmplrEy9I",
        "outputId": "8f4ce53e-93d9-4830-b97d-94292e586060"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfzTdMtvGE6w"
      },
      "source": [
        "And we see how the chat template transformed these conversations.\n",
        "\n",
        "**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "vhXv0xFMGNKE",
        "outputId": "f3e42a3f-e236-43ff-dcbc-652600ebecc2"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "834213762cce405bb754238979f95b55",
            "bf98f5fa9edd4c7fb39b1eebe48faa60",
            "31dad788294f4ba1aed116ec4cc057b5",
            "0714884f6fd84f5ca3b47ecfa809f01b",
            "7180bad9b47c49cf8b1cab0decd73f7c",
            "60d429a8a84a44d88f345a38d1d07ed8",
            "ed36beaba9c640e9896c439c53839520",
            "2d645794e4dc4a34a3aeb9458f8bbfc9",
            "d086bcf736dd41cda221fa5b36c96839",
            "58e07cd67b424bd884020295c3ec9021",
            "f4e277307b84418789b0dcefab3d1183"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "66e31549-98bf-4f43-aa1e-8a7f2d291231"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "import wandb\n",
        "import torch\n",
        "import gc\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Updated sweep config with overfitting prevention\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'name': 'eval/loss', 'goal': 'minimize'},  # Changed to eval/loss!\n",
        "    'parameters': {\n",
        "        'learning_rate': {'min': 5e-6, 'max': 2e-4},  # Lower LR range\n",
        "        'gradient_accumulation_steps': {'values': [4, 8]},\n",
        "        'lora_dropout': {'values': [0.05, 0.1]},  # Add dropout\n",
        "        'weight_decay': {'values': [0.01, 0.05]},  # Add weight decay\n",
        "    }\n",
        "}\n",
        "\n",
        "def train_func():\n",
        "    # Clean up memory before starting\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Record start time for duration tracking\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Initialize wandb (config is automatically provided by sweep)\n",
        "    # The agent automatically connects this to the sweep, but we specify project/entity for clarity\n",
        "    wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        entity=\"hayleyc-kth-royal-institute-of-technology\"\n",
        "    )\n",
        "    cfg = wandb.config\n",
        "    \n",
        "    # Reload model to ensure fresh weights for each run\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    \n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = 16,\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "        lora_alpha = 16,\n",
        "        lora_dropout = cfg.lora_dropout,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "        random_state = 3407,\n",
        "        use_rslora = False,\n",
        "        loftq_config = None,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = dataset,\n",
        "        eval_dataset = valid_dataset,  # Add validation dataset!\n",
        "        dataset_text_field = \"text\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = 1,\n",
        "            per_device_eval_batch_size = 1,\n",
        "            gradient_accumulation_steps = cfg.gradient_accumulation_steps,\n",
        "            warmup_steps = 10,\n",
        "            max_steps = 1500,  # Stop before overfitting (around step 2000)\n",
        "            learning_rate = cfg.learning_rate,\n",
        "            weight_decay = cfg.weight_decay,  # Regularization\n",
        "            fp16 = not is_bfloat16_supported(),\n",
        "            bf16 = is_bfloat16_supported(),\n",
        "            logging_steps = 10,\n",
        "            eval_strategy = \"steps\",  # Enable evaluation\n",
        "            eval_steps = 100,  # Evaluate every 100 steps\n",
        "            optim = \"adamw_8bit\",\n",
        "            output_dir = output_dir,\n",
        "            save_strategy = \"steps\",\n",
        "            save_steps = 500,\n",
        "            save_total_limit = 2,\n",
        "            load_best_model_at_end = True,  # Load best checkpoint\n",
        "            metric_for_best_model = \"eval_loss\",  # Use eval loss\n",
        "            greater_is_better = False,\n",
        "            report_to = \"wandb\",\n",
        "            seed = 3407\n",
        "        ),\n",
        "    )\n",
        "        \n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part = \"<|im_start|>user\\n\",\n",
        "        response_part = \"<|im_start|>assistant\\n\",\n",
        "    )\n",
        "    \n",
        "    trainer.train()\n",
        "    \n",
        "    # Clean up\n",
        "    del model\n",
        "    del trainer\n",
        "    del tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    end_time = datetime.now()\n",
        "    duration = end_time - start_time\n",
        "    print(f\"\\n‚è±Ô∏è Sweep run completed!\")\n",
        "    print(f\"   Start:    {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"   End:      {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"   Duration: {duration}\")\n",
        "\n",
        "# Initialize sweep\n",
        "# Initialize sweep with descriptive name\n",
        "if not USE_FULL_DATASET:\n",
        "    \n",
        "    sweep_id = wandb.sweep(\n",
        "        sweep_config, \n",
        "        project=WANDB_PROJECT,\n",
        "        entity=\"hayleyc-kth-royal-institute-of-technology\"\n",
        "    )\n",
        "\n",
        "    # Run 3 sweeps (optimized for 12-hour Colab session)\n",
        "    wandb.agent(\n",
        "        sweep_id, \n",
        "        function=train_func, \n",
        "        count=3, \n",
        "        project=WANDB_PROJECT,\n",
        "        entity=\"hayleyc-kth-royal-institute-of-technology\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéØ Final Training with Best Hyperparameters\n",
        "\n",
        "## When to run this section:\n",
        "1. ‚úÖ After all 3 sweep runs complete\n",
        "2. ‚úÖ After reviewing WandB to find best hyperparameters\n",
        "3. ‚úÖ Set `USE_FULL_DATASET = True` above\n",
        "\n",
        "## What this does:\n",
        "- Trains on **full 81k samples** (not 15% subsample)\n",
        "- Uses **best hyperparameters** from sweep\n",
        "- **Early stopping** to prevent overfitting\n",
        "- Automatically **uploads to Hugging Face** when done\n",
        "- Takes ~20-25 hours on T4 GPU (may stop earlier if converged)\n",
        "\n",
        "## üõë Early Stopping:\n",
        "Training will automatically stop if:\n",
        "- No improvement in eval/loss for 3 consecutive evaluations (1500 steps)\n",
        "- Minimum improvement threshold: 0.01\n",
        "- Prevents wasting compute on overfitting\n",
        "\n",
        "## ‚ö†Ô∏è Before running:\n",
        "1. Check WandB sweep results\n",
        "2. Best hyperparameters will be auto-fetched\n",
        "3. Ensure you have enough GPU time remaining\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only run this after sweeps complete and USE_FULL_DATASET = True\n",
        "if USE_FULL_DATASET:\n",
        "    from datetime import datetime\n",
        "    \n",
        "    final_training_start = datetime.now()\n",
        "    print(f\"üïê Final training started at: {final_training_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    print(\"üöÄ Starting final training with full dataset...\")\n",
        "    \n",
        "    # Check for existing checkpoint to resume from\n",
        "    import glob\n",
        "    checkpoints = glob.glob(f\"{output_dir}/checkpoint-*\")\n",
        "    resume_from_checkpoint = None\n",
        "    \n",
        "    if checkpoints:\n",
        "        # Sort by step number and get the latest\n",
        "        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
        "        resume_from_checkpoint = checkpoints[-1]\n",
        "        print(f\"üìÇ Found checkpoint: {resume_from_checkpoint}\")\n",
        "        print(\"   Resuming training from this checkpoint...\")\n",
        "    else:\n",
        "        print(\"üìÇ No checkpoint found. Starting fresh training.\")\n",
        "    \n",
        "    # Automatically get best hyperparameters from WandB sweep\n",
        "    print(\"\\nüìä Fetching best hyperparameters from WandB sweep...\")\n",
        "    \n",
        "    api = wandb.Api()\n",
        "    sweeps = api.project(\n",
        "        WANDB_PROJECT,\n",
        "        entity=\"hayleyc-kth-royal-institute-of-technology\"\n",
        "    ).sweeps()\n",
        "    \n",
        "    latest_sweep = list(sweeps)[0]\n",
        "    print(f\"Found sweep: {latest_sweep.name} ({latest_sweep.id})\")\n",
        "    \n",
        "    best_run = latest_sweep.best_run()\n",
        "    \n",
        "    if best_run:\n",
        "        BEST_LEARNING_RATE = best_run.config.get('learning_rate', 1e-4)\n",
        "        BEST_GRAD_ACCUM = best_run.config.get('gradient_accumulation_steps', 8)\n",
        "        BEST_LORA_DROPOUT = best_run.config.get('lora_dropout', 0.05)\n",
        "        BEST_WEIGHT_DECAY = best_run.config.get('weight_decay', 0.01)\n",
        "        \n",
        "        best_eval_loss = best_run.summary.get('eval/loss', 'N/A')\n",
        "        \n",
        "        print(f\"\\n‚úÖ Best hyperparameters from run '{best_run.name}':\")\n",
        "        print(f\"   Learning Rate: {BEST_LEARNING_RATE}\")\n",
        "        print(f\"   Gradient Accumulation: {BEST_GRAD_ACCUM}\")\n",
        "        print(f\"   LoRA Dropout: {BEST_LORA_DROPOUT}\")\n",
        "        print(f\"   Weight Decay: {BEST_WEIGHT_DECAY}\")\n",
        "        print(f\"   Best eval/loss: {best_eval_loss}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No best run found. Using default values.\")\n",
        "        BEST_LEARNING_RATE = 1e-4\n",
        "        BEST_GRAD_ACCUM = 8\n",
        "        BEST_LORA_DROPOUT = 0.05\n",
        "        BEST_WEIGHT_DECAY = 0.01\n",
        "    \n",
        "    # Initialize WandB\n",
        "    wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        entity=\"hayleyc-kth-royal-institute-of-technology\",\n",
        "        name=f\"final-training-{TIMESTAMP}\",\n",
        "        resume=\"allow\",  # Allow resuming if run exists\n",
        "        config={\n",
        "            'learning_rate': BEST_LEARNING_RATE,\n",
        "            'gradient_accumulation_steps': BEST_GRAD_ACCUM,\n",
        "            'lora_dropout': BEST_LORA_DROPOUT,\n",
        "            'weight_decay': BEST_WEIGHT_DECAY,\n",
        "            'dataset_size': len(dataset),\n",
        "            'source': 'auto-selected from sweep',\n",
        "            'resumed_from_checkpoint': resume_from_checkpoint is not None,\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Load model\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    \n",
        "    # Add LoRA\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = 16,\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "        lora_alpha = 16,\n",
        "        lora_dropout = BEST_LORA_DROPOUT,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "        random_state = 3407,\n",
        "        use_rslora = False,\n",
        "        loftq_config = None,\n",
        "    )\n",
        "    \n",
        "    # Create trainer with checkpoint settings\n",
        "    trainer = SFTTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = dataset,\n",
        "        eval_dataset = valid_dataset,\n",
        "        dataset_text_field = \"text\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "        callbacks = [EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)],\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = 2,\n",
        "            per_device_eval_batch_size = 2,\n",
        "            gradient_accumulation_steps = BEST_GRAD_ACCUM,\n",
        "            warmup_steps = 10,\n",
        "            num_train_epochs = 1,\n",
        "            learning_rate = BEST_LEARNING_RATE,\n",
        "            weight_decay = BEST_WEIGHT_DECAY,\n",
        "            fp16 = not is_bfloat16_supported(),\n",
        "            bf16 = is_bfloat16_supported(),\n",
        "            logging_steps = 10,\n",
        "            eval_strategy = \"steps\",\n",
        "            eval_steps = 500,\n",
        "            # Early stopping configuration\n",
        "            early_stopping_patience = 3,  # Stop if no improvement for 3 evals\n",
        "            early_stopping_threshold = 0.01,  # Minimum improvement threshold\n",
        "            optim = \"adamw_8bit\",\n",
        "            output_dir = output_dir,\n",
        "            # Checkpoint settings\n",
        "            save_strategy = \"steps\",\n",
        "            save_steps = 500,  # Save every 500 steps\n",
        "            save_total_limit = 3,  # Keep last 3 checkpoints\n",
        "            # Resume settings\n",
        "            load_best_model_at_end = True,\n",
        "            metric_for_best_model = \"eval_loss\",\n",
        "            greater_is_better = False,\n",
        "            report_to = \"wandb\",\n",
        "            seed = 3407\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part = \"<|im_start|>user\\n\",\n",
        "        response_part = \"<|im_start|>assistant\\n\",\n",
        "    )\n",
        "    \n",
        "    # Train (will resume from checkpoint if found)\n",
        "    print(\"\\nüéØ Training started...\")\n",
        "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "    \n",
        "    # Get final metrics\n",
        "    final_metrics = trainer.evaluate()\n",
        "    final_eval_loss = final_metrics['eval_loss']\n",
        "    print(f\"\\n‚úÖ Training complete! Final eval_loss: {final_eval_loss:.4f}\")\n",
        "    \n",
        "    # Check if model is good enough to upload\n",
        "    if final_eval_loss < 1.5:  # Reasonable threshold\n",
        "        print(\"\\nüì§ Model quality is good! Uploading to Hugging Face...\")\n",
        "        print(\"   This will take ~45-60 minutes for all formats.\\n\")\n",
        "        \n",
        "        # 1. Upload LoRA adapters (fastest, for experimentation)\n",
        "        print(\"[1/3] Uploading LoRA adapters...\")\n",
        "        model.push_to_hub(\"hayitsmaddy/mamamadal\", token=HF_TOKEN)\n",
        "        tokenizer.push_to_hub(\"hayitsmaddy/mamamadal\", token=HF_TOKEN)\n",
        "        print(\"      ‚úÖ LoRA adapters uploaded (~200MB)\")\n",
        "        \n",
        "        # 2. Upload merged 4bit (best for web service deployment)\n",
        "        print(\"\\n[2/3] Creating and uploading merged 4bit model (recommended for web service)...\")\n",
        "        model.push_to_hub_merged(\n",
        "            \"hayitsmaddy/mamamadal\",\n",
        "            tokenizer,\n",
        "            save_method=\"merged_4bit\",\n",
        "            token=HF_TOKEN\n",
        "        )\n",
        "        print(\"      ‚úÖ Merged 4bit model uploaded (~4GB)\")\n",
        "        print(\"      üí° This is a standalone model, no base model needed!\")\n",
        "        \n",
        "        # 3. Upload GGUF formats (for Ollama, LM Studio, local deployment)\n",
        "        print(\"\\n[3/3] Creating and uploading GGUF formats (for local deployment)...\")\n",
        "        model.push_to_hub_gguf(\n",
        "            \"hayitsmaddy/mamamadal\",\n",
        "            tokenizer,\n",
        "            quantization_method=[\"q4_k_m\", \"q5_k_m\"],  # Good balance\n",
        "            token=HF_TOKEN\n",
        "        )\n",
        "        print(\"      ‚úÖ GGUF formats uploaded (Q4_K_M ~4GB, Q5_K_M ~5GB)\")\n",
        "        print(\"      üí° Use these for Ollama, LM Studio, or llama.cpp\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üéâ ALL UPLOADS COMPLETE!\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"\\nüì¶ Your model repository now contains:\")\n",
        "        print(\"   1. LoRA adapters (for experimentation)\")\n",
        "        print(\"   2. Merged 4bit model (for web service deployment)\")\n",
        "        print(\"   3. GGUF Q4_K_M & Q5_K_M (for local deployment)\")\n",
        "        print(\"\\nüîó View at: https://huggingface.co/hayitsmaddy/mamamadal\")\n",
        "        print(\"\\nüí° For your web chatbot service:\")\n",
        "        print(\"   - Use the merged 4bit model\")\n",
        "        print(\"   - Load with: AutoModelForCausalLM.from_pretrained('hayitsmaddy/mamamadal')\")\n",
        "        print(\"   - No need to load base model separately!\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Model quality not ideal (eval_loss: {final_eval_loss:.4f})\")\n",
        "        print(\"   Recommended threshold: < 1.5\")\n",
        "        print(\"   Uploading only LoRA adapters for review...\")\n",
        "        model.push_to_hub(\"hayitsmaddy/mamamadal\", token=HF_TOKEN)\n",
        "        tokenizer.push_to_hub(\"hayitsmaddy/mamamadal\", token=HF_TOKEN)\n",
        "        print(\"\\nüí° Consider:\")\n",
        "        print(\"   - Adjusting hyperparameters\")\n",
        "        print(\"   - Training for more steps\")\n",
        "        print(\"   - Checking for data quality issues\")\n",
        "    \n",
        "    \n",
        "    # Get final metrics\n",
        "    \n",
        "    print(\"\\nüì§ Uploading to Hugging Face Hub...\")\n",
        "    \n",
        "    wandb.finish()\n",
        "    wandb.finish()\n",
        "    \n",
        "    final_training_end = datetime.now()\n",
        "    final_duration = final_training_end - final_training_start\n",
        "    \n",
        "    print(\"\\nüéâ Final training complete!\")\n",
        "    print(f\"\\n‚è±Ô∏è Training Time:\")\n",
        "    print(f\"   Start:    {final_training_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"   End:      {final_training_end.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"   Duration: {final_duration}\")\n",
        "    hours = final_duration.total_seconds() / 3600\n",
        "    print(f\"   ({hours:.2f} hours)\")\n",
        "    \n",
        "    # Model and tokenizer are now available globally for inference\n",
        "    print(\"\\nüí° Model and tokenizer are ready for testing!\")\n",
        "    print(\"   You can now run the inference cells below.\")\n",
        "    \n",
        "    # Now trainer exists and you can run verification cells below\n",
        "    print(\"\\nüí° Tip: You can now run the verification cells below to check training masking.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping final training. Set USE_FULL_DATASET = True to run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify model and tokenizer are available\n",
        "if 'model' in globals() and 'tokenizer' in globals():\n",
        "    print(\"‚úÖ Model and tokenizer are loaded and ready!\")\n",
        "    print(f\"   Model type: {type(model).__name__}\")\n",
        "    print(f\"   Tokenizer type: {type(tokenizer).__name__}\")\n",
        "    print(\"\\nüí° You can now run inference cells to test the model.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Model or tokenizer not found.\")\n",
        "    print(\"   Make sure final training completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä How to Verify Model Performance\n",
        "\n",
        "### During Sweeps (15% dataset):\n",
        "\n",
        "**Check WandB Dashboard:**\n",
        "1. Go to your sweep page\n",
        "2. Look at **Parallel Coordinates** plot\n",
        "3. Compare `eval/loss` across runs\n",
        "4. **Good signs:**\n",
        "   - `eval/loss` decreasing steadily\n",
        "   - `train/loss` and `eval/loss` stay close\n",
        "   - No sudden spikes or divergence\n",
        "\n",
        "**Best run criteria:**\n",
        "- ‚úÖ Lowest `eval/loss`\n",
        "- ‚úÖ Stable training (no spikes)\n",
        "- ‚úÖ Small gap between train/eval loss\n",
        "\n",
        "### After Final Training (Full dataset):\n",
        "\n",
        "**Quantitative checks:**\n",
        "1. **Final `eval/loss`**: Should be < 1.0 (lower is better)\n",
        "2. **Train vs Eval gap**: Should be < 0.2 (not overfitting)\n",
        "3. **Loss curve**: Smooth downward trend\n",
        "\n",
        "**Qualitative checks (run inference below):**\n",
        "1. Test with sample questions\n",
        "2. Check if responses are coherent\n",
        "3. Verify it follows instructions\n",
        "4. Compare to base model\n",
        "\n",
        "### üéØ Decision Guide:\n",
        "\n",
        "**Upload to HF if:**\n",
        "- ‚úÖ `eval/loss` < 1.0\n",
        "- ‚úÖ Responses look good in inference tests\n",
        "- ‚úÖ No overfitting (train/eval gap small)\n",
        "\n",
        "**Don't upload if:**\n",
        "- ‚ùå `eval/loss` > 1.5\n",
        "- ‚ùå Responses are incoherent\n",
        "- ‚ùå Large train/eval gap (overfitting)\n",
        "- ‚ùå Loss didn't decrease much\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "We verify masking is actually done:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Verifying Training Data Masking\n",
        "\n",
        "**What's happening here:**\n",
        "These cells verify that `train_on_responses_only` is working correctly.\n",
        "\n",
        "**Why this matters:**\n",
        "- We only want to train on the **assistant's responses**, not the user's questions\n",
        "- This prevents the model from learning to generate questions instead of answers\n",
        "- The system/instruction prompts should be **masked** (ignored during training)\n",
        "\n",
        "**What to look for:**\n",
        "1. **First cell**: Shows the full conversation with special tokens\n",
        "2. **Second cell**: Shows what parts are actually trained on (masked parts appear as spaces)\n",
        "3. **Result**: Only the assistant's responses should be visible in the second output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è Training Verification Cells (Currently Disabled)\n",
        "\n",
        "**Note**: These cells are commented out because `trainer` only exists inside the sweep function.\n",
        "\n",
        "**To verify training masking:**\n",
        "1. After sweeps complete, set `USE_FULL_DATASET = True`\n",
        "2. The final training section will create a `trainer` object\n",
        "3. Then you can run these verification cells\n",
        "\n",
        "**What these cells do:**\n",
        "- Show the full conversation with tokens\n",
        "- Show which parts are masked (not trained on)\n",
        "- Verify only assistant responses are being trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtsMVtlkUhja"
      },
      "outputs": [],
      "source": [
        "# Verify training data format (run after final training)\n",
        "# Shows what the model sees during training\n",
        "if 'trainer' in globals():\n",
        "    print(\"Sample training example:\")\n",
        "    print(tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Run this after final training completes (trainer needs to exist)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enWUM0jV-jV"
      },
      "source": [
        "We can see the System and Instruction prompts are successfully masked!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíª GPU Memory Check\n",
        "\n",
        "Run this cell to check GPU memory usage.\n",
        "Useful for:\n",
        "- Verifying you have enough memory\n",
        "- Debugging OOM errors\n",
        "- Optimizing batch sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üß™ Model Testing & Inference\n",
        "\n",
        "## ‚ö†Ô∏è Run these cells AFTER final training completes!\n",
        "\n",
        "These cells test your fine-tuned model with sample prompts.\n",
        "\n",
        "### What to check:\n",
        "1. **Response quality**: Are answers coherent and relevant?\n",
        "2. **Instruction following**: Does it follow the prompt?\n",
        "3. **Comparison**: How does it compare to base model?\n",
        "\n",
        "### Testing workflow:\n",
        "1. Run inference examples below\n",
        "2. Try your own prompts\n",
        "3. If quality is good ‚Üí Model is ready for deployment!\n",
        "4. If quality is poor ‚Üí Check eval/loss, consider retraining\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "# Run this after final training completes\n",
        "if 'model' not in globals():\n",
        "    print(\"‚ö†Ô∏è Model not loaded yet!\")\n",
        "    print(\"   Set USE_FULL_DATASET = True and run final training first.\")\n",
        "else:\n",
        "    # Test 1: Basic inference (no streaming)\n",
        "    from unsloth.chat_templates import get_chat_template\n",
        "    \n",
        "    tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-3\")\n",
        "    FastLanguageModel.for_inference(model)  # Enable 2x faster inference\n",
        "    \n",
        "    # Test prompt\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Continue the fibonacci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=64,\n",
        "        use_cache=True,\n",
        "        temperature=1.5,\n",
        "        min_p=0.1\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.batch_decode(outputs)\n",
        "    print(\"Model response:\")\n",
        "    print(response[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "# Run this after final training completes\n",
        "if 'model' not in globals():\n",
        "    print(\"‚ö†Ô∏è Model not loaded yet!\")\n",
        "    print(\"   Set USE_FULL_DATASET = True and run final training first.\")\n",
        "else:\n",
        "    # Test 2: Streaming inference (see tokens as they generate)\n",
        "    from transformers import TextStreamer\n",
        "    \n",
        "    FastLanguageModel.for_inference(model)\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Explain what fine-tuning is in simple terms.\"},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "    print(\"Model response (streaming):\")\n",
        "    _ = model.generate(\n",
        "        input_ids=inputs,\n",
        "        streamer=text_streamer,\n",
        "        max_new_tokens=128,\n",
        "        use_cache=True,\n",
        "        temperature=1.5,\n",
        "        min_p=0.1\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ When to Upload to Hugging Face Hub\n",
        "\n",
        "### ‚ö†Ô∏è IMPORTANT: Do NOT run this cell during hyperparameter sweeps!\n",
        "\n",
        "**Upload timing guide:**\n",
        "\n",
        "### ‚ùå DON'T Upload:\n",
        "- During the 3 sweep runs (these are experiments, not final models)\n",
        "- After each individual sweep run\n",
        "- Before you've identified the best hyperparameters\n",
        "\n",
        "### ‚úÖ DO Upload:\n",
        "1. **After sweep completes** and you've identified best hyperparameters\n",
        "2. **After final training** with `USE_FULL_DATASET = True`\n",
        "3. **When eval/loss is good** and you're satisfied with results\n",
        "\n",
        "### üìã Upload Workflow:\n",
        "```\n",
        "1. Run 3 sweep experiments ‚Üí Don't upload\n",
        "2. Check WandB, find best hyperparameters ‚Üí Don't upload\n",
        "3. Set USE_FULL_DATASET = True ‚Üí Don't upload\n",
        "4. Train with best hyperparameters on full data ‚Üí Don't upload\n",
        "5. Verify final model performance ‚Üí NOW UPLOAD! ‚úÖ\n",
        "```\n",
        "\n",
        "### üéØ What this cell does:\n",
        "- **LoRA adapters only**: Uploads just the trained adapter weights (~100-200MB)\n",
        "- **Not the full model**: Base model stays on HF, adapters are loaded on top\n",
        "- **Repository**: `hayitsmaddy/mamamadal`\n",
        "- **Private**: You can change this in repo settings\n",
        "\n",
        "### üí° To use this cell:\n",
        "1. Make sure you're happy with the model's performance\n",
        "2. Uncomment the lines (remove `# ` at the start)\n",
        "3. Run the cell\n",
        "4. Check https://huggingface.co/hayitsmaddy/mamamadal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üíæ Model Saving Options\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANT: Only run these AFTER final training completes!\n",
        "\n",
        "The final training cell above already uploads LoRA adapters automatically.\n",
        "These cells are for:\n",
        "1. **Local backups** (save to disk)\n",
        "2. **Different formats** (merged models, GGUF for deployment)\n",
        "3. **Manual control** (if you want to save specific versions)\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Option 1: LoRA Adapters (Recommended)\n",
        "\n",
        "**What it saves:**\n",
        "- Only the trained adapter weights (~100-200MB)\n",
        "- Requires base model to use\n",
        "\n",
        "**When to use:**\n",
        "- ‚úÖ Sharing your fine-tune\n",
        "- ‚úÖ Quick uploads\n",
        "- ‚úÖ Experimenting with different adapters\n",
        "\n",
        "**Note:** Final training already does this automatically!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "# Run this after final training completes\n",
        "if 'model' not in globals():\n",
        "    print(\"‚ö†Ô∏è Model not loaded yet!\")\n",
        "    print(\"   Set USE_FULL_DATASET = True and run final training first.\")\n",
        "else:\n",
        "    # Test 2: Streaming inference (see tokens as they generate)\n",
        "    from transformers import TextStreamer\n",
        "    \n",
        "    FastLanguageModel.for_inference(model)\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Explain what fine-tuning is in simple terms.\"},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "    print(\"Model response (streaming):\")\n",
        "    _ = model.generate(\n",
        "        input_ids=inputs,\n",
        "        streamer=text_streamer,\n",
        "        max_new_tokens=128,\n",
        "        use_cache=True,\n",
        "        temperature=1.5,\n",
        "        min_p=0.1\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Advanced: Merging and Converting Models\n",
        "\n",
        "**‚ö†Ô∏è Only run these after final training, not during sweeps!**\n",
        "\n",
        "### What are these options?\n",
        "\n",
        "**LoRA Adapters (Default - Recommended)**\n",
        "- ‚úÖ Small file size (~100-200MB)\n",
        "- ‚úÖ Fast upload\n",
        "- ‚úÖ Easy to share\n",
        "- ‚ùå Requires base model to use\n",
        "\n",
        "**Merged 16bit (Full Model)**\n",
        "- ‚úÖ Standalone model, no base model needed\n",
        "- ‚úÖ Full precision\n",
        "- ‚ùå Large file size (~16GB)\n",
        "- ‚ùå Slow upload\n",
        "\n",
        "**Merged 4bit (Quantized)**\n",
        "- ‚úÖ Smaller than 16bit (~4GB)\n",
        "- ‚úÖ Faster inference\n",
        "- ‚ùå Slight quality loss\n",
        "\n",
        "**GGUF (For llama.cpp)**\n",
        "- ‚úÖ Works with Ollama, LM Studio, llama.cpp\n",
        "- ‚úÖ Multiple quantization options\n",
        "- ‚úÖ Best for local deployment\n",
        "\n",
        "### üí° Recommendation:\n",
        "1. **For sharing**: Upload LoRA adapters (lightest, easiest)\n",
        "2. **For deployment**: Convert to GGUF q4_k_m (good balance)\n",
        "3. **For production**: Merge to 16bit (best quality)\n",
        "\n",
        "### üéØ To use:\n",
        "Change `if False:` to `if True:` for the option you want\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "# OPTIONAL: Additional merged model formats\n",
        "The merged 4bit model is already uploaded automatically!\n",
        "Only run these if you specifically need 16bit precision\n",
        "\n",
        "# Merge to 16bit (Full precision, ~16GB) - OPTIONAL\n",
        "if False:\n",
        "    print(\"Uploading merged 16bit model...\")\n",
        "    model.push_to_hub_merged(\"hayitsmaddy/mamamadal\", tokenizer, save_method=\"merged_16bit\", token=HF_TOKEN)\n",
        "    print(\"‚úÖ Uploaded 16bit merged model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hayitsmaddy/mamamadal\", tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hayitsmaddy/mamamadal\", tokenizer, save_method = \"merged_4bit\", token = HF_TOKEN)\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hayitsmaddy/mamamadal\", tokenizer, save_method = \"lora\", token = HF_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "# OPTIONAL: Additional GGUF formats\n",
        "Q4_K_M and Q5_K_M are already uploaded automatically!\n",
        "Only run these if you specifically need higher quality GGUF\n",
        "\n",
        "# Q8_0 (8-bit, ~8GB, excellent quality) - OPTIONAL\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hayitsmaddy/mamamadal\", tokenizer, quantization_method=\"q8_0\", token=HF_TOKEN)\n",
        "    print(\"‚úÖ Uploaded Q8_0 GGUF\")\n",
        "\n",
        "# F16 (16-bit, ~16GB, maximum quality) - OPTIONAL\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hayitsmaddy/mamamadal\", tokenizer, quantization_method=\"f16\", token=HF_TOKEN)\n",
        "    print(\"‚úÖ Uploaded F16 GGUF\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hayitsmaddy/mamamadal\", tokenizer, token = HF_TOKEN)\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hayitsmaddy/mamamadal\", tokenizer, quantization_method = \"f16\", token = HF_TOKEN)\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hayitsmaddy/mamamadal\", tokenizer, quantization_method = \"q4_k_m\", token = HF_TOKEN)\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hayitsmaddy/mamamadal\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = HF_TOKEN, # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NjQ-bHeHIhN"
      },
      "outputs": [],
      "source": [
        "# Optional: Save GGUF locally (only run after final training)\n",
        "# This is already uploaded to HuggingFace automatically!\n",
        "# Only run this if you want a local copy\n",
        "\n",
        "if 'model' in globals() and USE_FULL_DATASET:\n",
        "    print(\"Saving GGUF format locally...\")\n",
        "    model.save_pretrained_gguf(\n",
        "        \"model-gguf\",\n",
        "        tokenizer,\n",
        "        quantization_method=\"q4_k_m\"\n",
        "    )\n",
        "    print(\"‚úÖ Saved to: model-gguf/\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping local GGUF save (model not trained yet or not using full dataset)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ What Gets Uploaded Automatically\n",
        "\n",
        "After final training completes successfully, the notebook automatically uploads:\n",
        "\n",
        "### 1. LoRA Adapters (~200MB)\n",
        "- Lightweight adapter weights\n",
        "- For experimentation and sharing\n",
        "\n",
        "### 2. Merged 4bit Model (~4GB) ‚≠ê\n",
        "- **Use this for your web chatbot service!**\n",
        "- Standalone model (no base model needed)\n",
        "- Good quality, fast inference\n",
        "- Perfect for CPU inference on Streamlit/Gradio\n",
        "\n",
        "### 3. GGUF Formats (~4-5GB)\n",
        "- Q4_K_M and Q5_K_M\n",
        "- For Ollama, LM Studio, llama.cpp\n",
        "- Optimized for CPU inference\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Using Your Model\n",
        "\n",
        "### In Streamlit/Gradio (CPU inference):\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'hayitsmaddy/mamamadal',\n",
        "    device_map='cpu'  # CPU inference\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained('hayitsmaddy/mamamadal')\n",
        "```\n",
        "\n",
        "### With GGUF (llama.cpp):\n",
        "```bash\n",
        "# Download GGUF from HuggingFace\n",
        "# Use with Ollama or llama.cpp for fast CPU inference\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html).\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "10. [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "11. [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "12. [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0714884f6fd84f5ca3b47ecfa809f01b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58e07cd67b424bd884020295c3ec9021",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f4e277307b84418789b0dcefab3d1183",
            "value": "‚Äá81000/81000‚Äá[02:02&lt;00:00,‚Äá986.05‚Äáexamples/s]"
          }
        },
        "178b697d1b674b37bbdb1ef796da3b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d2be03f2b2d4951bfdf870bc7d677f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "215fef8284e748a39829d266b0a242f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d645794e4dc4a34a3aeb9458f8bbfc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31dad788294f4ba1aed116ec4cc057b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d645794e4dc4a34a3aeb9458f8bbfc9",
            "max": 81000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d086bcf736dd41cda221fa5b36c96839",
            "value": 81000
          }
        },
        "38f1f9c97517438c96d3635f19bea369": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_481d140728da40108b1b44c700274261",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_83dc1227b69b448e8d24f16e2f9dfea5",
            "value": "Map:‚Äá100%"
          }
        },
        "444392dd785b42778bab7966c8bf8416": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "481d140728da40108b1b44c700274261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523d2b505ef443e6b29f52453d6fd608": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e07cd67b424bd884020295c3ec9021": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60d429a8a84a44d88f345a38d1d07ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65feeb6f753647e492375e2cd855496e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_178b697d1b674b37bbdb1ef796da3b0f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d82a8fd44d1f4cb983b45c02073bef34",
            "value": "‚Äá2/2‚Äá[00:28&lt;00:00,‚Äá13.50s/it]"
          }
        },
        "7180bad9b47c49cf8b1cab0decd73f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f8601ca5a8342d78ac8dbb66accb814": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb0742afecdd41b8842f039146523551",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1d2be03f2b2d4951bfdf870bc7d677f8",
            "value": "‚Äá81000/81000‚Äá[03:58&lt;00:00,‚Äá381.60‚Äáexamples/s]"
          }
        },
        "834213762cce405bb754238979f95b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf98f5fa9edd4c7fb39b1eebe48faa60",
              "IPY_MODEL_31dad788294f4ba1aed116ec4cc057b5",
              "IPY_MODEL_0714884f6fd84f5ca3b47ecfa809f01b"
            ],
            "layout": "IPY_MODEL_7180bad9b47c49cf8b1cab0decd73f7c"
          }
        },
        "83dc1227b69b448e8d24f16e2f9dfea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "859345a12282417db419c1d2260bd534": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_444392dd785b42778bab7966c8bf8416",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e395b822390f4b8c897bfa34a56a0e36",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "861e2487a72645bd96e79729ddf58839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ebe513193ad438f86fb9f1fa821a0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38f1f9c97517438c96d3635f19bea369",
              "IPY_MODEL_919368811ac242cd9b9257376e12874b",
              "IPY_MODEL_7f8601ca5a8342d78ac8dbb66accb814"
            ],
            "layout": "IPY_MODEL_523d2b505ef443e6b29f52453d6fd608"
          }
        },
        "919368811ac242cd9b9257376e12874b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e722c1d8c8f1498fa16bdf5cc65d4b61",
            "max": 81000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e6dd230283642a195e288db0798f04b",
            "value": 81000
          }
        },
        "9e6dd230283642a195e288db0798f04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b391f372a4d547079d00d51a383ad370": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_861e2487a72645bd96e79729ddf58839",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d30ec7f1cacf47559ec299b5da611a7c",
            "value": 2
          }
        },
        "bb0742afecdd41b8842f039146523551": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf98f5fa9edd4c7fb39b1eebe48faa60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60d429a8a84a44d88f345a38d1d07ed8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ed36beaba9c640e9896c439c53839520",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"
          }
        },
        "d086bcf736dd41cda221fa5b36c96839": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d30ec7f1cacf47559ec299b5da611a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d82a8fd44d1f4cb983b45c02073bef34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e395b822390f4b8c897bfa34a56a0e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e722c1d8c8f1498fa16bdf5cc65d4b61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7b2b25d5114ffe9e4e01c65d027b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_859345a12282417db419c1d2260bd534",
              "IPY_MODEL_b391f372a4d547079d00d51a383ad370",
              "IPY_MODEL_65feeb6f753647e492375e2cd855496e"
            ],
            "layout": "IPY_MODEL_215fef8284e748a39829d266b0a242f6"
          }
        },
        "ed36beaba9c640e9896c439c53839520": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4e277307b84418789b0dcefab3d1183": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
